
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator
from pyspark.mllib.evaluation import MulticlassMetrics
from pyspark.sql.types import DoubleType
import json

#  initialise SPARK
spark = SparkSession.builder \
    .appName("ğŸ¤– Pandemic Model Trainer v2") \
    .config("spark.sql.legacy.timeParserPolicy", "LEGACY") \
    .getOrCreate()

# uploading data
try:
    df = spark.read.csv("data/covid_dataset.csv", header=True, inferSchema=True)
    print("âœ… Dataset chargÃ© avec succÃ¨s (lignes:", df.count(), ")")
except Exception as e:
    print(f"âŒ Erreur de chargement: {str(e)}")
    spark.stop()
    exit()

df = df.withColumn("Active", df.Confirmed - (df.Deaths + df.Recovered)) \
       .withColumn("HighRisk", (df.Deaths / df.Confirmed > 0.05).cast("integer")) \
       .drop("Deaths / 100 Cases")  

# split  
train, test = df.randomSplit([0.8, 0.2], seed=42)

#data analyse
print("\nğŸ” Distribution des classes (test set):")
test.groupBy("HighRisk").count().show()

# FEATURES CONFIGURATION 
features_reg = ["Confirmed", "Active"]
features_clf = ["Confirmed", "Active"]

# ğŸ”§ pipeline regression
assembler_reg = VectorAssembler(inputCols=features_reg, outputCol="features")
rf = RandomForestRegressor(
    featuresCol="features",
    labelCol="New cases",
    numTrees=15,
    maxDepth=5,
    seed=42
)

# ğŸ”§ PIPELINE classification 
assembler_clf = VectorAssembler(inputCols=features_clf, outputCol="features")
lr = LogisticRegression(
    featuresCol="features",
    labelCol="HighRisk",
    maxIter=10,
    regParam=0.1
)

#  training
print("\nğŸ”® EntraÃ®nement des modÃ¨les...")
model_reg = rf.fit(assembler_reg.transform(train))
model_clf = lr.fit(assembler_clf.transform(train))

# ğŸ“Š evaluation 
print("\nğŸ§ª Ã‰valuation rigoureuse:")

# RÃ©gression
reg_pred = model_reg.transform(assembler_reg.transform(test))
rmse = RegressionEvaluator(
    labelCol="New cases", 
    metricName="rmse"
).evaluate(reg_pred)

# classification
clf_pred = model_clf.transform(assembler_clf.transform(test))

# conversion pour les mÃ©triques
clf_pred = clf_pred.withColumn("label", clf_pred["HighRisk"].cast(DoubleType())) \
                   .withColumn("prediction", clf_pred["prediction"].cast(DoubleType()))

# advanced metrics
evaluator = MulticlassClassificationEvaluator(labelCol="label")
accuracy = evaluator.evaluate(clf_pred, {evaluator.metricName: "accuracy"})
f1 = evaluator.evaluate(clf_pred, {evaluator.metricName: "f1"})

# matrice de confusion
metrics = MulticlassMetrics(clf_pred.select("prediction", "label").rdd)
conf_matrix = metrics.confusionMatrix().toArray()

# ğŸ’¾ saving
metrics_data = {
    "rmse": float(rmse),
    "accuracy": float(accuracy),
    "f1_score": float(f1),
    "confusion_matrix": conf_matrix.tolist()
}

with open("models/metrics.json", "w") as f:
    json.dump(metrics_data, f, indent=2)

print(f"""
ğŸ¯ RÃ‰SULTATS VALIDES:
--------------------
[ğŸ”¥ RÃ©gression]
- RMSE: {rmse:.2f}

[ğŸ¯ Classification]
- Accuracy: {accuracy:.2%}
- F1-Score: {f1:.2%}

Matrice de confusion:
{conf_matrix}
""")

# â• ANALYSE DES FEATURES 
print("\nğŸ” Importance des features (RÃ©gression):")
print(model_reg.featureImportances)

spark.stop()